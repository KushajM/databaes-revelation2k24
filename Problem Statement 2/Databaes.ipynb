{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8023349,"sourceType":"datasetVersion","datasetId":4728119},{"sourceId":8023379,"sourceType":"datasetVersion","datasetId":4728140},{"sourceId":8048239,"sourceType":"datasetVersion","datasetId":4745962},{"sourceId":8048255,"sourceType":"datasetVersion","datasetId":4745977},{"sourceId":8048325,"sourceType":"datasetVersion","datasetId":4746026}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow_text\nimport tensorflow_text as text  # Registers the ops.\n\n# After running this cell, we have to restart the Kernel and after restarting run this cell once again!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T03:40:56.338862Z","iopub.execute_input":"2024-04-07T03:40:56.339211Z","iopub.status.idle":"2024-04-07T03:41:25.456921Z","shell.execute_reply.started":"2024-04-07T03:40:56.339180Z","shell.execute_reply":"2024-04-07T03:41:25.456038Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow_text in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: tensorflow-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_text) (0.16.1)\nRequirement already satisfied: tensorflow<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_text) (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.51.1)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow_text)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.13.0->tensorflow_text) (2.15.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.2.2)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.0.5\n    Uninstalling keras-3.0.5:\n      Successfully uninstalled keras-3.0.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\n","output_type":"stream"},{"name":"stderr","text":"2024-04-07 03:41:14.153217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-07 03:41:14.153351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-07 03:41:14.264693: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install contractions\n!pip install transformers\n!pip install spacy","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:41:42.200475Z","iopub.execute_input":"2024-04-07T03:41:42.201352Z","iopub.status.idle":"2024-04-07T03:42:22.130706Z","shell.execute_reply.started":"2024-04-07T03:41:42.201312Z","shell.execute_reply":"2024-04-07T03:42:22.129637Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting textsearch>=0.0.21 (from contractions)\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting anyascii (from textsearch>=0.0.21->contractions)\n  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\nCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\nDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import spacy\nimport numpy as np\nimport pandas as pd\nimport contractions\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('/kaggle/input/memes-data/data.csv')\ndf = data.sample(frac=0.8, random_state=42)\ntest = data.drop(df.index)\ndf.head(7)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:43:14.361070Z","iopub.execute_input":"2024-04-07T03:43:14.362191Z","iopub.status.idle":"2024-04-07T03:43:15.263189Z","shell.execute_reply.started":"2024-04-07T03:43:14.362152Z","shell.execute_reply":"2024-04-07T03:43:15.262117Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                   text            img  label  \\\n6252               i did nazi that bird fly by did you?  img/63915.png      0   \n4684                      best holiday gift for muslims  img/37864.png      0   \n1731              found a picture of your mother's womb  img/90625.png      1   \n4742                                 bruce jenner's cat  img/12807.png      0   \n4521  i do not want to hear anymore about gun contro...  img/26049.png      0   \n6340  normie: \"your meme's are racist and offensive,...  img/24537.png      1   \n576   it's time to kill the british and dump tea int...  img/35216.png      1   \n\n                                                Caption  \n6252   a black and white photo of a man with a mustache  \n4684  there is a man and woman sitting at a table wi...  \n1731   there is a baby that is laying down on a blanket  \n4742  arafed cat sitting in front of a building with...  \n4521  arafed image of a man in a suit and tie with a...  \n6340    there is a kid that is playing with a spongebob  \n576   someone told the british all duping a into the...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>img</th>\n      <th>label</th>\n      <th>Caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6252</th>\n      <td>i did nazi that bird fly by did you?</td>\n      <td>img/63915.png</td>\n      <td>0</td>\n      <td>a black and white photo of a man with a mustache</td>\n    </tr>\n    <tr>\n      <th>4684</th>\n      <td>best holiday gift for muslims</td>\n      <td>img/37864.png</td>\n      <td>0</td>\n      <td>there is a man and woman sitting at a table wi...</td>\n    </tr>\n    <tr>\n      <th>1731</th>\n      <td>found a picture of your mother's womb</td>\n      <td>img/90625.png</td>\n      <td>1</td>\n      <td>there is a baby that is laying down on a blanket</td>\n    </tr>\n    <tr>\n      <th>4742</th>\n      <td>bruce jenner's cat</td>\n      <td>img/12807.png</td>\n      <td>0</td>\n      <td>arafed cat sitting in front of a building with...</td>\n    </tr>\n    <tr>\n      <th>4521</th>\n      <td>i do not want to hear anymore about gun contro...</td>\n      <td>img/26049.png</td>\n      <td>0</td>\n      <td>arafed image of a man in a suit and tie with a...</td>\n    </tr>\n    <tr>\n      <th>6340</th>\n      <td>normie: \"your meme's are racist and offensive,...</td>\n      <td>img/24537.png</td>\n      <td>1</td>\n      <td>there is a kid that is playing with a spongebob</td>\n    </tr>\n    <tr>\n      <th>576</th>\n      <td>it's time to kill the british and dump tea int...</td>\n      <td>img/35216.png</td>\n      <td>1</td>\n      <td>someone told the british all duping a into the...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import cv2\nimg_path = \"/kaggle/input/memes-dataset/Dataset/\"\ndef generate_img(img_path):\n    path = str(img_path)\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (256, 256))\n\n    return img\n\nimgs=[]\nfor index, row in df.iterrows():\n    path = img_path + row['img']\n    imgs.append(generate_img(path))\n    \nimgs = np.array(imgs)\nimgs.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:43:18.962987Z","iopub.execute_input":"2024-04-07T03:43:18.963371Z","iopub.status.idle":"2024-04-07T03:46:09.344576Z","shell.execute_reply.started":"2024-04-07T03:43:18.963339Z","shell.execute_reply":"2024-04-07T03:46:09.343676Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(8000, 256, 256, 3)"},"metadata":{}}]},{"cell_type":"code","source":"import re\nimport string \n\nnlp = spacy.load('en_core_web_sm')\nslang_df = pd.read_csv('/kaggle/input/slangs-data/slangs_translation.csv')\nslangs = dict(slang_df.values)\nexclude = string.punctuation\n\ndef cleaning(data):\n    \n    df = data.copy()\n    \n    def remove_html_tags(text): return re.sub(r'<.*?>', '', text)\n    df['text'] = df['text'].apply(remove_html_tags)\n    \n    def expand_contractions(text): return contractions.fix(text)\n    df['text'] = df['text'].apply(expand_contractions)\n\n    def remove_web_urls(text): return re.sub(r'https?:\\/\\/\\S+', ' ', text)\n    df['text'] = df['text'].apply(remove_web_urls)\n    \n    def remove_links(text): return re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', ' ', text)\n    df['text'] = df['text'].apply(remove_links)\n    \n    df['text'] = df['text'].str.lower()\n    \n    def chat_conversion(text):\n        new_text = []\n        for word in text.split():\n            if word.lower() in slangs:\n                new_text.append(slangs[word])\n            else:\n                new_text.append(word)\n        return \" \".join(new_text)\n    \n    df['text'] = df['text'].apply(chat_conversion)\n    \n    def remove_tags(text): return re.sub(r'@\\w*', ' ' , text)\n    df['text'] = df['text'].apply(remove_tags)\n    \n    def remove_hashtags(text): return re.sub(r'#\\w*', ' ' , text)\n    df['text'] = df['text'].apply(remove_hashtags)\n    \n    def remove_apostrophe(text): return re.sub(r\"'s\\b\", \"\", text)\n    df['text'] = df['text'].apply(remove_apostrophe)\n\n    def remove_special_chars(text): return re.sub(r\"[^a-zA-Z0-9\\s]\", ' ', text)\n    df['text'] = df['text'].apply(remove_special_chars)\n    \n    def remove_number(text): return re.sub(r'[\\d]', ' ', text)\n    df['text'] = df['text'].apply(remove_number)\n    \n    def remove_punc(text): return ''.join([c for c in text if c not in exclude])\n    df['text'] = df['text'].apply(remove_punc)\n    \n    def lemmatize(text):\n        doc = nlp(text)\n        lemmatized_tokens = [token.lemma_ for token in doc]\n        return ' '.join(lemmatized_tokens)\n    \n    df['text'] = df['text'].apply(lemmatize)\n\n    return df\n\n# Apply function to dataframe\ndf = cleaning(df)\ndf.head(7)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:46:56.645025Z","iopub.execute_input":"2024-04-07T03:46:56.645535Z","iopub.status.idle":"2024-04-07T03:47:56.876078Z","shell.execute_reply.started":"2024-04-07T03:46:56.645506Z","shell.execute_reply":"2024-04-07T03:47:56.874945Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                   text            img  label  \\\n6252                  I do nazi that bird fly by do you  img/63915.png      0   \n4684                       good holiday gift for muslim  img/37864.png      0   \n1731                 find a picture of your mother womb  img/90625.png      1   \n4742                                   bruce jenner cat  img/12807.png      0   \n4521  I do not want time out hear anymore about gun ...  img/26049.png      0   \n6340  normie    your meme be racist and offensive   ...  img/24537.png      1   \n576   information technology be tear in my eye time ...  img/35216.png      1   \n\n                                                Caption  \n6252   a black and white photo of a man with a mustache  \n4684  there is a man and woman sitting at a table wi...  \n1731   there is a baby that is laying down on a blanket  \n4742  arafed cat sitting in front of a building with...  \n4521  arafed image of a man in a suit and tie with a...  \n6340    there is a kid that is playing with a spongebob  \n576   someone told the british all duping a into the...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>img</th>\n      <th>label</th>\n      <th>Caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6252</th>\n      <td>I do nazi that bird fly by do you</td>\n      <td>img/63915.png</td>\n      <td>0</td>\n      <td>a black and white photo of a man with a mustache</td>\n    </tr>\n    <tr>\n      <th>4684</th>\n      <td>good holiday gift for muslim</td>\n      <td>img/37864.png</td>\n      <td>0</td>\n      <td>there is a man and woman sitting at a table wi...</td>\n    </tr>\n    <tr>\n      <th>1731</th>\n      <td>find a picture of your mother womb</td>\n      <td>img/90625.png</td>\n      <td>1</td>\n      <td>there is a baby that is laying down on a blanket</td>\n    </tr>\n    <tr>\n      <th>4742</th>\n      <td>bruce jenner cat</td>\n      <td>img/12807.png</td>\n      <td>0</td>\n      <td>arafed cat sitting in front of a building with...</td>\n    </tr>\n    <tr>\n      <th>4521</th>\n      <td>I do not want time out hear anymore about gun ...</td>\n      <td>img/26049.png</td>\n      <td>0</td>\n      <td>arafed image of a man in a suit and tie with a...</td>\n    </tr>\n    <tr>\n      <th>6340</th>\n      <td>normie    your meme be racist and offensive   ...</td>\n      <td>img/24537.png</td>\n      <td>1</td>\n      <td>there is a kid that is playing with a spongebob</td>\n    </tr>\n    <tr>\n      <th>576</th>\n      <td>information technology be tear in my eye time ...</td>\n      <td>img/35216.png</td>\n      <td>1</td>\n      <td>someone told the british all duping a into the...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, Flatten, Dense, Reshape, Dropout, Add\nfrom tensorflow.keras.models import Model\n\ndef residual_block(X,filters):\n    # Retrieve Filters\n    F1, F2 = filters\n    # Saving the input value.we need this later to add to the output. \n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU(alpha=0.1)(X)\n\n    # Third component of main path \n    X = Conv2D(filters = F2, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n    X = BatchNormalization()(X)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation \n    X = Add()([X, X_shortcut])\n    X = LeakyReLU(alpha=0.1)(X)\n    return X","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:48:13.556879Z","iopub.execute_input":"2024-04-07T03:48:13.557593Z","iopub.status.idle":"2024-04-07T03:48:13.569308Z","shell.execute_reply.started":"2024-04-07T03:48:13.557559Z","shell.execute_reply":"2024-04-07T03:48:13.568356Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Reshape\n\n\ndef create_bert_classification_model(dropout_rate=0.2, dense_layer_sizes=[128, 32], learning_rate=2e-6):\n    base_model = ResNet50(\n        weights='imagenet',\n        input_shape=(256, 256, 3),  # Input shape of the images (height, width, channels)\n        include_top=False  # Exclude the top classification layers\n    )\n\n    # Freeze the base model's layers to prevent them from being trained\n    base_model.trainable = False\n    \n    img_input = tf.keras.layers.Input(shape=(256,256,3), dtype=tf.float32, name=\"img_input\")\n    \n    x = base_model(img_input)\n\n    # Add a convolutional layer with 512 filters, followed by batch normalization and LeakyReLU activation\n    x = Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.1)(x)\n\n    # Add a custom residual block (function residual_block needs to be defined separately)\n    x = residual_block(x, [64, 128])\n    x = Flatten()(x)\n    \n    \n    # Input layers for two text columns\n    text_input1 = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text_input1\")\n    text_input2 = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text_input2\")\n\n    # Preprocessing for text column 1\n    preprocessor1 = hub.KerasLayer(\n        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n    encoder_inputs1 = preprocessor1(text_input1)\n    \n    # Preprocessing for text column 2\n    preprocessor2 = hub.KerasLayer(\n        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n    encoder_inputs2 = preprocessor2(text_input2)\n    \n    # BERT Encoder for each text column\n    encoder1 = hub.KerasLayer(\n        \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\",\n        trainable=True)\n    encoder2 = hub.KerasLayer(\n        \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\",\n        trainable=True)\n    outputs1 = encoder1(encoder_inputs1)\n    outputs2 = encoder2(encoder_inputs2)\n\n    # Extract sequence outputs from BERT for each text column\n    sequence_output1 = outputs1[\"sequence_output\"]  # [batch_size, seq_length, 768].\n    sequence_output2 = outputs2[\"sequence_output\"]  # [batch_size, seq_length, 768].\n    \n    # Bidirectional LSTM layer for each text column\n    lstm_output1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(sequence_output1)\n    lstm_output2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(sequence_output2)\n\n\n    concatenated_output = tf.keras.layers.Concatenate(axis=1)([x, lstm_output1, lstm_output2])\n    hidden = tf.keras.layers.Dense(128, activation='relu')(concatenated_output)\n    \n    # Output layer\n    output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(hidden)\n\n    # Define and compile model\n    model = tf.keras.Model(inputs=[img_input, text_input1, text_input2], outputs=[output_layer])\n    \n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                  loss=tf.keras.losses.BinaryCrossentropy(),\n                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n    \n    return model\n\n# Example usage\nmodel = create_bert_classification_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:51:26.148603Z","iopub.execute_input":"2024-04-07T03:51:26.149384Z","iopub.status.idle":"2024-04-07T03:51:56.649161Z","shell.execute_reply.started":"2024-04-07T03:51:26.149351Z","shell.execute_reply":"2024-04-07T03:51:56.648231Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n img_input (InputLayer)      [(None, 256, 256, 3)]        0         []                            \n                                                                                                  \n resnet50 (Functional)       (None, 8, 8, 2048)           2358771   ['img_input[0][0]']           \n                                                          2                                       \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 8, 8, 128)            2359424   ['resnet50[0][0]']            \n                                                                                                  \n batch_normalization_3 (Bat  (None, 8, 8, 128)            512       ['conv2d_3[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n leaky_re_lu_3 (LeakyReLU)   (None, 8, 8, 128)            0         ['batch_normalization_3[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 8, 8, 64)             8256      ['leaky_re_lu_3[0][0]']       \n                                                                                                  \n batch_normalization_4 (Bat  (None, 8, 8, 64)             256       ['conv2d_4[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n leaky_re_lu_4 (LeakyReLU)   (None, 8, 8, 64)             0         ['batch_normalization_4[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 8, 8, 128)            8320      ['leaky_re_lu_4[0][0]']       \n                                                                                                  \n batch_normalization_5 (Bat  (None, 8, 8, 128)            512       ['conv2d_5[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n text_input1 (InputLayer)    [(None,)]                    0         []                            \n                                                                                                  \n text_input2 (InputLayer)    [(None,)]                    0         []                            \n                                                                                                  \n add_1 (Add)                 (None, 8, 8, 128)            0         ['batch_normalization_5[0][0]'\n                                                                    , 'leaky_re_lu_3[0][0]']      \n                                                                                                  \n keras_layer_4 (KerasLayer)  {'input_mask': (None, 128)   0         ['text_input1[0][0]']         \n                             , 'input_word_ids': (None,                                           \n                              128),                                                               \n                              'input_type_ids': (None,                                            \n                             128)}                                                                \n                                                                                                  \n keras_layer_5 (KerasLayer)  {'input_mask': (None, 128)   0         ['text_input2[0][0]']         \n                             , 'input_word_ids': (None,                                           \n                              128),                                                               \n                              'input_type_ids': (None,                                            \n                             128)}                                                                \n                                                                                                  \n leaky_re_lu_5 (LeakyReLU)   (None, 8, 8, 128)            0         ['add_1[0][0]']               \n                                                                                                  \n keras_layer_6 (KerasLayer)  {'encoder_outputs': [(None   1094822   ['keras_layer_4[0][0]',       \n                             , 128, 768),                 41         'keras_layer_4[0][1]',       \n                              (None, 128, 768),                      'keras_layer_4[0][2]']       \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768)],                                                  \n                              'default': (None, 768),                                             \n                              'sequence_output': (None,                                           \n                              128, 768),                                                          \n                              'pooled_output': (None, 7                                           \n                             68)}                                                                 \n                                                                                                  \n keras_layer_7 (KerasLayer)  {'default': (None, 768),     1094822   ['keras_layer_5[0][0]',       \n                              'encoder_outputs': [(None   41         'keras_layer_5[0][1]',       \n                             , 128, 768),                            'keras_layer_5[0][2]']       \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768)],                                                  \n                              'sequence_output': (None,                                           \n                              128, 768),                                                          \n                              'pooled_output': (None, 7                                           \n                             68)}                                                                 \n                                                                                                  \n flatten_1 (Flatten)         (None, 8192)                 0         ['leaky_re_lu_5[0][0]']       \n                                                                                                  \n bidirectional_2 (Bidirecti  (None, 256)                  918528    ['keras_layer_6[0][14]']      \n onal)                                                                                            \n                                                                                                  \n bidirectional_3 (Bidirecti  (None, 256)                  918528    ['keras_layer_7[0][14]']      \n onal)                                                                                            \n                                                                                                  \n concatenate_1 (Concatenate  (None, 8704)                 0         ['flatten_1[0][0]',           \n )                                                                   'bidirectional_2[0][0]',     \n                                                                     'bidirectional_3[0][0]']     \n                                                                                                  \n dense_1 (Dense)             (None, 128)                  1114240   ['concatenate_1[0][0]']       \n                                                                                                  \n output (Dense)              (None, 1)                    129       ['dense_1[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 247880899 (945.59 MB)\nTrainable params: 224292545 (855.61 MB)\nNon-trainable params: 23588354 (89.98 MB)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(\n    [imgs, df[\"text\"], df[\"Caption\"]],  \n    df[\"label\"],  # Target column\n    epochs=5,\n    batch_size=8\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:52:15.054216Z","iopub.execute_input":"2024-04-07T03:52:15.055130Z","iopub.status.idle":"2024-04-07T04:22:00.315863Z","shell.execute_reply.started":"2024-04-07T03:52:15.055096Z","shell.execute_reply":"2024-04-07T04:22:00.315007Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/5\n1000/1000 [==============================] - 421s 345ms/step - loss: 0.7470 - binary_accuracy: 0.5656\nEpoch 2/5\n1000/1000 [==============================] - 340s 340ms/step - loss: 0.5915 - binary_accuracy: 0.6950\nEpoch 3/5\n1000/1000 [==============================] - 340s 340ms/step - loss: 0.4962 - binary_accuracy: 0.7675\nEpoch 4/5\n1000/1000 [==============================] - 340s 340ms/step - loss: 0.4193 - binary_accuracy: 0.8161\nEpoch 5/5\n1000/1000 [==============================] - 340s 340ms/step - loss: 0.3548 - binary_accuracy: 0.8539\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(\n    [imgs, df[\"text\"], df[\"Caption\"]],  \n    df[\"label\"],  # Target column\n    epochs=2,\n    batch_size=8\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:26:07.898021Z","iopub.execute_input":"2024-04-07T04:26:07.898971Z","iopub.status.idle":"2024-04-07T04:37:31.840885Z","shell.execute_reply.started":"2024-04-07T04:26:07.898934Z","shell.execute_reply":"2024-04-07T04:37:31.839942Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/2\n1000/1000 [==============================] - 340s 340ms/step - loss: 0.2968 - binary_accuracy: 0.8844\nEpoch 2/2\n1000/1000 [==============================] - 340s 340ms/step - loss: 0.2428 - binary_accuracy: 0.9135\n","output_type":"stream"}]},{"cell_type":"code","source":"# EVALUATION\n\nimport cv2\nimg_path = \"/kaggle/input/memes-dataset/Dataset/\"\ndef generate_img(img_path):\n    path = str(img_path)\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (256, 256))\n\n    return img\n\n\ntest_img =[]\nfor index, row in test.iterrows():\n    path = img_path + row['img']\n    test_img.append(generate_img(path))\n    \ntest_img = np.array(test_img)\n\ntest = cleaning(test)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:38:07.655566Z","iopub.execute_input":"2024-04-07T04:38:07.656340Z","iopub.status.idle":"2024-04-07T04:38:48.916191Z","shell.execute_reply.started":"2024-04-07T04:38:07.656305Z","shell.execute_reply":"2024-04-07T04:38:48.915347Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Assuming you have your test data in text_column1_test, text_column2_test, and target_test\ncolumn1 = test['text']\ncolumn2 = test['Caption']\ntarget = test['label']\n\n# Evaluate the model on test data\ntest_loss, test_accuracy = model.evaluate([test_img, column1, column2], target, verbose=2)\n\n# Predict on test data\ny_pred = model.predict([test_img, column1, column2])\n\nbinary_threshold = 0.5\nbinary_y_pred = (y_pred >= binary_threshold).astype(int)\n\nauc_score = roc_auc_score(target, binary_y_pred)\n\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(\"AUC Score\", round(auc_score, 4))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:38:58.037246Z","iopub.execute_input":"2024-04-07T04:38:58.037688Z","iopub.status.idle":"2024-04-07T04:39:57.735717Z","shell.execute_reply.started":"2024-04-07T04:38:58.037650Z","shell.execute_reply":"2024-04-07T04:39:57.734645Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"63/63 - 29s - loss: 0.7090 - binary_accuracy: 0.6875 - 29s/epoch - 459ms/step\n63/63 [==============================] - 29s 460ms/step\nTest Loss: 0.7089815735816956\nTest Accuracy: 0.6875\nAUC Score 0.6628\n","output_type":"stream"}]},{"cell_type":"code","source":"# EVALUATION ON COVID19 DATASET\n\nimport pandas as pd\ndata = pd.read_csv('/kaggle/input/brain-dead-harmful-data-for-covid19/harmful_data_for_covid19/haemful_meme_for_covid19.csv', encoding= 'ISO-8859-1')\ndata.head(7)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:40:17.605471Z","iopub.execute_input":"2024-04-07T04:40:17.605837Z","iopub.status.idle":"2024-04-07T04:40:17.663106Z","shell.execute_reply.started":"2024-04-07T04:40:17.605809Z","shell.execute_reply":"2024-04-07T04:40:17.662177Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                  image       labels  \\\n0  covid_memes_1466.png      harmful   \n1   covid_memes_471.png      harmful   \n2  covid_memes_1413.png  not harmful   \n3  covid_memes_1712.png      harmful   \n4  covid_memes_3734.png      harmful   \n5  covid_memes_5557.png      harmful   \n6  covid_memes_2166.png  not harmful   \n\n                                                text  \n0  \"You are what you eat\"\\nSome dude in wuhan:\\nI...  \n1  COVID FACTS:\\nPer the CDC elfective 07/10/2020...  \n2                        NO00, DO NOT\\nWEH BE LIKE\\n  \n3                         WHEN YOUR BARBER\\nCOUGHS\\n  \n4  The Volaile Mermaid\\nTFW you hear that Trump a...  \n5  Colin Snoka\\nTrump after receiving his Covid d...  \n6  BLINDED BY\\nTHE RIGHT\\nMERICA\\nREVVED UP\\nLIKE...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>labels</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>covid_memes_1466.png</td>\n      <td>harmful</td>\n      <td>\"You are what you eat\"\\nSome dude in wuhan:\\nI...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>covid_memes_471.png</td>\n      <td>harmful</td>\n      <td>COVID FACTS:\\nPer the CDC elfective 07/10/2020...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>covid_memes_1413.png</td>\n      <td>not harmful</td>\n      <td>NO00, DO NOT\\nWEH BE LIKE\\n</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>covid_memes_1712.png</td>\n      <td>harmful</td>\n      <td>WHEN YOUR BARBER\\nCOUGHS\\n</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>covid_memes_3734.png</td>\n      <td>harmful</td>\n      <td>The Volaile Mermaid\\nTFW you hear that Trump a...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>covid_memes_5557.png</td>\n      <td>harmful</td>\n      <td>Colin Snoka\\nTrump after receiving his Covid d...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>covid_memes_2166.png</td>\n      <td>not harmful</td>\n      <td>BLINDED BY\\nTHE RIGHT\\nMERICA\\nREVVED UP\\nLIKE...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n\n# https://huggingface.co/Salesforce/blip-image-captioning-large\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n\nfor index, row in data.iterrows():\n    img = row.img\n    root_path = \"/kaggle/input/brain-dead-harmful-data-for-covid19/harmful_data_for_covid19/\"\n    raw_image = Image.open(root_path+str(img)).convert('RGB')\n    \n    # unconditional image captioning\n    inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n    out = model.generate(**inputs, max_new_tokens=40)\n    caption = processor.decode(out[0], skip_special_tokens=True)\n    data.at[index, 'Caption'] = caption\n    raw_image = Image.open(root_path+str(img)).convert('RGB')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:42:44.450803Z","iopub.execute_input":"2024-04-07T04:42:44.451561Z","iopub.status.idle":"2024-04-07T04:42:47.244801Z","shell.execute_reply.started":"2024-04-07T04:42:44.451526Z","shell.execute_reply":"2024-04-07T04:42:47.243184Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"GPU is available\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      7\u001b[0m processor \u001b[38;5;241m=\u001b[39m BlipProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip-image-captioning-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBlipForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalesforce/blip-image-captioning-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     11\u001b[0m     img \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mimg\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 26.12 MiB is free. Process 2956 has 15.87 GiB memory in use. Of the allocated memory 341.62 MiB is allocated by PyTorch, and 4.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 26.12 MiB is free. Process 2956 has 15.87 GiB memory in use. Of the allocated memory 341.62 MiB is allocated by PyTorch, and 4.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"import cv2\nimg_path = \"/kaggle/input/memes-dataset/Dataset/\"\ndef generate_img(img_path):\n    path = str(img_path)\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (256, 256))\n\n    return img\n\ncovid =[]\nfor index, row in df.iterrows():\n    path = img_path + row['img']\n    covid.append(generate_img(path))\n    \ncovid = np.array(covid)\ncovid.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have your test data in text_column1_test, text_column2_test, and target_test\ncolumn1 = data['text']\ncolumn2 = data['Caption']\ntarget = data['label']\n\n# Evaluate the model on test data\ntest_loss, test_accuracy = model.evaluate([covid, column1, column2], target, verbose=2)\n\n# Predict on test data\ny_pred = model.predict([covid, column1, column2])\n\nbinary_threshold = 0.5\nbinary_y_pred = (y_pred >= binary_threshold).astype(int)\n\nauc_score = roc_auc_score(target, binary_y_pred)\n\nprintf(\"COVID 19 data:\")\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(\"AUC Score\", round(auc_score, 4))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EVALUATION ON COVID19 DATASET\n\nimport pandas as pd\ndata = pd.read_csv('/kaggle/input/brain-dead-harmful-data-for-politics/harmful_meme_politics/harmful_meme_for_politics.csv', encoding= 'ISO-8859-1')\ndata.head(7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n\n# https://huggingface.co/Salesforce/blip-image-captioning-large\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n\nfor index, row in data.iterrows():\n    img = row.img\n    root_path = \"/kaggle/input/brain-dead-harmful-data-for-politics/harmful_meme_politics/\"\n    raw_image = Image.open(root_path+str(img)).convert('RGB')\n    \n    # unconditional image captioning\n    inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n    out = model.generate(**inputs, max_new_tokens=40)\n    caption = processor.decode(out[0], skip_special_tokens=True)\n    data.at[index, 'Caption'] = caption\"\n    raw_image = Image.open(root_path+str(img)).convert('RGB')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimg_path = \"/kaggle/input/memes-dataset/Dataset/\"\ndef generate_img(img_path):\n    path = str(img_path)\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (256, 256))\n\n    return img\n\npolitics =[]\nfor index, row in df.iterrows():\n    path = img_path + row['img']\n    politics.append(generate_img(path))\n    \npolitics = np.array(politics)\npolitics.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have your test data in text_column1_test, text_column2_test, and target_test\ncolumn1 = data['text']\ncolumn2 = data['Caption']\ntarget = data['label']\n\n# Evaluate the model on test data\ntest_loss, test_accuracy = model.evaluate([covid, column1, column2], target, verbose=2)\n\n# Predict on test data\ny_pred = model.predict([covid, column1, column2])\n\nbinary_threshold = 0.5\nbinary_y_pred = (y_pred >= binary_threshold).astype(int)\n\nauc_score = roc_auc_score(target, binary_y_pred)\n\nprintf(\"COVID 19 data:\")\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(\"AUC Score\", round(auc_score, 4))","metadata":{},"execution_count":null,"outputs":[]}]}